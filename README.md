# HÆ°á»›ng dáº«n khá»Ÿi Ä‘á»™ng vÃ  tÆ°Æ¡ng tÃ¡c há»‡ thá»‘ng

## 1. Khá»Ÿi Ä‘á»™ng há»‡ thá»‘ng cÆ¡ báº£n

**ThÃ nh pháº§n:**

* **lakeFS**: Quáº£n lÃ½ phiÃªn báº£n dá»¯ liá»‡u
* **PostgreSQL**: LÆ°u metadata cho lakeFS
* **MinIO**: Object store tÆ°Æ¡ng thÃ­ch S3

**BÆ°á»›c thá»±c hiá»‡n:**

```bash
sudo systemctl stop postgresql  # Táº¯t PostgreSQL náº¿u Ä‘ang chiáº¿m cá»•ng 5432
docker compose up -d           # Khá»Ÿi cháº¡y cÃ¡c container
```

---

## 2. TÆ°Æ¡ng tÃ¡c qua CLI

### 2.1. DÃ¹ng `mc` (MinIO Client)

```bash
docker compose exec -it mc sh  # VÃ o container mc
exit                            # ThoÃ¡t
```

**ThÃªm alias:**

```bash
mc alias set myminio http://minio:9000 admin Admin12345
```

**Lá»‡nh cÆ¡ báº£n:**

```bash
mc alias list                               # Kiá»ƒm tra alias
mc alias remove myminio                    # XÃ³a alias (náº¿u sai)
mc mb myminio/mybucket                     # Táº¡o bucket
mc ls myminio                              # Liá»‡t kÃª bucket
mc ls myminio/mybucket                     # Liá»‡t kÃª file trong bucket
mc cp /data/file.csv myminio/mybucket      # Upload file
```

> âœ¨ **ThÆ° má»¥c `/data` trong container `mc` Ä‘Ã£ Ä‘Æ°á»£c mount tá»« `data/` host.**

---

### 2.2. DÃ¹ng `lakectl` (CLI lakeFS)

```bash
docker compose exec -it lakefs sh   # VÃ o container lakefs
lakectl --help                      # HÆ°á»›ng dáº«n
exit                                # ThoÃ¡t
```

> ğŸ”¹ File `lakectl.yaml` Ä‘Ã£ Ä‘Æ°á»£c mount sáºµn, khÃ´ng cáº§n Ä‘áº·t tay config.

---

## 3. TÆ°Æ¡ng tÃ¡c vá»›i lakeFS

### Táº¡o repo:

```bash
lakectl repo create lakefs://myrepo s3://mybucket
```

### XÃ³a repo:

```bash
lakectl repo delete lakefs://myrepo
```

### Xem dá»¯ liá»‡u trong nhÃ¡nh:

```bash
lakectl fs ls lakefs://myrepo/main/
```

### Upload file:

```bash
lakectl fs upload --source /upload/students.csv lakefs://myrepo/main/students.csv
```

### Upload cáº£ folder:

```bash
lakectl fs upload --recursive --source /upload/ lakefs://myrepo/main/rawdata/
```

!\[upload files]\(images/upload files.png)

### XÃ³a file / thÆ° má»¥c:

```bash
lakectl fs rm lakefs://myrepo/main/students.csv
lakectl fs rm --recursive lakefs://myrepo/main/rawdata/
```

### Commit thay Ä‘á»•i:

```bash
lakectl commit lakefs://myrepo/dev -m "upd"
```

!\[branch commit]\(images/branch commit.png)

### NhÃ¡nh (branch):

```bash
lakectl branch create lakefs://myrepo/dev --source lakefs://myrepo/main
```

!\[create new branch]\(images/create new branch.png)

### Táº¡o thÆ° má»¥c báº±ng upload file:

```bash
echo "ghi chu" > issue.txt
lakectl fs upload --source issue.txt lakefs://myrepo/dev/processed-data/issue.txt
```

### Reset thay Ä‘á»•i chÆ°a commit:

```bash
lakectl branch reset lakefs://myrepo/dev
```

### Rollback commit:

```bash
lakectl branch revert lakefs://myrepo/main <commit-id>
```

### Kiá»ƒm tra commit hiá»‡n táº¡i:

```bash
lakectl branch show lakefs://myrepo/main
```

### So sÃ¡nh vÃ  merge:

```bash
lakectl diff lakefs://myrepo/dev
lakectl merge lakefs://myrepo/dev lakefs://myrepo/main
```

![merge](images/merge.png)

---

## 4. Káº¿t ná»‘i Apache Spark

Spark Ä‘á»c ghi dá»¯ liá»‡u tá»« lakeFS thÃ´ng qua connector `hadoop-lakefs`, tÆ°Æ¡ng thÃ­ch vá»›i giao thá»©c `lakefs://`.

> âœ… Táº¥t cáº£ cáº¥u hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c Ä‘áº·t trong `docker-compose.yml`

### ğŸ”§ Cáº¥u hÃ¬nh quan trá»ng:

**Data plane (`s3a://`) â€“ Ä‘i qua MinIO:**

```env
fs.s3a.endpoint=http://minio:9000
fs.s3a.path.style.access=true
fs.s3a.access.key=${MINIO_ROOT_USER}
fs.s3a.secret.key=${MINIO_ROOT_PASSWORD}
fs.s3a.connection.ssl.enabled=false
```

**Metadata plane (`lakefs://`) â€“ xá»­ lÃ½ version:**

```env
fs.lakefs.impl=io.lakefs.LakeFSFileSystem
fs.lakefs.endpoint=http://lakefs:8000/api/v1
fs.lakefs.access.key=${LAKEFS_ACCESS_KEY}
fs.lakefs.secret.key=${LAKEFS_SECRET_KEY}
```

### ğŸ“ Mount thÆ° viá»‡n vÃ  jobs:

* `./jobs:/opt/bitnami/spark/jobs:ro`: mount script xá»­ lÃ½ nhÆ° `vdt1.py`, `vdt2.py`, `vdt3.py`
* `./jars/io-lakefs-hadoop-lakefs-assembly-0.2.5.jar`: connector há»— trá»£ `lakefs://`

---

## 5. Xá»­ lÃ½ vá»›i Spark

### ğŸ§ª VÃ­ dá»¥: `jobs/vdt1.py`

```python
# VDT'25 coding test Q1
# Ná»™i dung yÃªu cáº§u:

#     Láº¥y danh sÃ¡ch khÃ³a há»c báº¡n Ä‘Äƒng kÃ½ tham gia (theo tÃªn hoáº·c email)
#     Schema báº£ng Ä‘áº§u ra gá»“m: full_name, mail, course_title, platform, start_date, end_date
#     Äá»‹nh dáº¡ng trÆ°á»ng date: yyyyMMdd

# YÃªu cáº§u Ä‘áº§u ra:

#     LÆ°u file output vá»›i Ä‘á»‹nh dáº¡ng csv khÃ´ng cÃ³ header, phÃ¢n cÃ¡ch trÆ°á»ng báº±ng dáº¥u pháº©y ( , )
#     File output cÃ³ tÃªn 'result_p1_1'
#--------------------------------------------
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import *
import os

# khoi tao SparkSession
spark = SparkSession.builder\
        .appName("lakeFS pySpark processing")\
        .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000")\
        .config("spark.hadoop.fs.s3a.path.style.access", "true")\
        .config("spark.hadoop.fs.s3a.access.key", os.getenv("MINIO_ROOT_USER"))\
        .config("spark.hadoop.fs.s3a.secret.key", os.getenv("MINIO_ROOT_PASSWORD"))\
        .config("spark.hadoop.fs.lakefs.impl", "io.lakefs.LakeFSFileSystem")\
        .config("spark.hadoop.fs.lakefs.endpoint", "http://lakefs:8000/api/v1")\
        .config("spark.hadoop.fs.lakefs.access.key", os.getenv("LAKEFS_ACCESS_KEY"))\
        .config("spark.hadoop.fs.lakefs.secret.key", os.getenv("LAKEFS_SECRET_KEY"))\
        .getOrCreate()

# doc du lieu tho tu thu muc rawdata cua nhanh dev
df_students = spark.read\
                .option("header", True)\
                .option("inferSchema", True)\
                .csv("lakefs://myrepo/dev/rawdata/students.csv")

df_learning_courses = spark.read\
                .orc("lakefs://myrepo/dev/rawdata/learning_courses.orc")

# xu ly du lieu
result1 = df_students.join(df_learning_courses, df_students['student_id'] == df_learning_courses['student_id'])
result1 = result1.select('full_name', 'mail', 'course_title', 'platform', 'start_date', 'end_date')

# ghi du lieu da xu ly ra thu muc processed-data cua nhanh dev
result1.write.mode('overwrite')\
    .format('csv').option('header', False)\
    .save("lakefs://myrepo/dev/processed-data/result_p1_1.csv")
```

### âš™ï¸ Cháº¡y job:

```bash
docker compose exec spark-master spark-submit \
  --master spark://spark-master:7077 \
  --packages io.lakefs:hadoop-lakefs-assembly:0.2.5 \
  /opt/bitnami/spark/jobs/vdt1.py
```

!\[káº¿t quáº£ spark]\(images/spark result.png)

> âœ¨ TÆ°Æ¡ng tá»± vá»›i `jobs/vdt2.py` náº¿u cÃ³ logic khÃ¡c (vÃ­ dá»¥ aggregate, join, etc.)

---

## 6. Delta Lake

Spark há»— trá»£ Delta format náº¿u thÃªm thÆ° viá»‡n tÆ°Æ¡ng á»©ng:

### Phá»¥ thuá»™c:

```bash
--packages io.delta:delta-core_2.12:2.4.0
```

### Táº¡o SparkSession:

```python
spark = SparkSession.builder \
    .appName("DeltaLake Integration") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .getOrCreate()
```

### Ghi vÃ  Ä‘á»c:

```python
df.write.format("delta").save("s3a://mybucket/delta-table")
delta_df = spark.read.format("delta").load("s3a://mybucket/delta-table")
```

### Time travel:

```python
spark.read.format("delta").option("versionAsOf", 0).load("s3a://mybucket/delta-table")
```

---

## Cáº¥u trÃºc dá»± Ã¡n

```text
demo/
â”œâ”€â”€ data/               # Dá»¯ liá»‡u upload
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env                # ThÃ´ng tin credentials
â”œâ”€â”€ lakectl.yaml        # Cáº¥u hÃ¬nh CLI lakectl
â”œâ”€â”€ jobs/               # ThÆ° má»¥c chá»©a file xá»­ lÃ½ Spark
â”‚   â”œâ”€â”€ vdt1.py
|   â”œâ”€â”€ vdt2.py
â”‚   â””â”€â”€ vdt3.py
â”œâ”€â”€ images/             # áº¢nh minh hoáº¡
â””â”€â”€ README.md           # HÆ°á»›ng dáº«n há»‡ thá»‘ng
```

---