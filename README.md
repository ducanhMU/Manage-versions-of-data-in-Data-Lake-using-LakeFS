# HÆ°á»›ng dáº«n khá»Ÿi Ä‘á»™ng vÃ  tÆ°Æ¡ng tÃ¡c há»‡ thá»‘ng

## 1. Khá»Ÿi Ä‘á»™ng há»‡ thá»‘ng cÆ¡ báº£n

**ThÃ nh pháº§n:**

* **lakeFS**: Quáº£n lÃ½ phiÃªn báº£n dá»¯ liá»‡u
* **PostgreSQL**: LÆ°u metadata cho lakeFS
* **MinIO**: Object store tÆ°Æ¡ng thÃ­ch S3

**BÆ°á»›c thá»±c hiá»‡n:**

```bash
sudo systemctl stop postgresql  # Táº¯t PostgreSQL náº¿u Ä‘ang chiáº¿m cá»•ng 5432

docker compose up -d           # Khá»Ÿi cháº¡y cÃ¡c container
```

---

## 2. TÆ°Æ¡ng tÃ¡c qua CLI

### 2.1. DÃ¹ng `mc` (MinIO Client)

```bash
docker compose exec -it mc sh  # VÃ o container mc
exit                            # ThoÃ¡t
```

**ThÃªm alias:**

```bash
mc alias set myminio http://minio:9000 admin Admin12345
```

**Lá»‡nh cÆ¡ báº£n:**

```bash
mc alias list                     # Kiá»ƒm tra alias
mc alias remove myminio          # XÃ³a alias (náº¿u sai)
mc mb myminio/mybucket           # Táº¡o bucket
mc ls myminio                    # Liá»‡t kÃª bucket
mc ls myminio/mybucket           # Liá»‡t kÃª file trong bucket
mc cp /data/file.csv myminio/mybucket # Upload file
```

> âœ¨ **ThÆ° má»¥c `/data` trong container `mc` Ä‘Ã£ Ä‘Æ°á»£c mount tá»« `data/` host.**

---

### 2.2. DÃ¹ng `lakectl` (CLI lakeFS)

```bash
docker compose exec -it lakefs sh   # VÃ o container lakefs
exit                                # ThoÃ¡t
lakectl --help                      # HÆ°á»›ng dáº«n
```

> ğŸ”¹ File `lakectl.yaml` Ä‘Ã£ Ä‘Æ°á»£c mount sáºµn, khÃ´ng cáº§n Ä‘áº·t tay config.

---

## 3. TÆ°Æ¡ng tÃ¡c vá»›i lakeFS

### Táº¡o repo:

```bash
lakectl repo create lakefs://myrepo s3://mybucket
```

### XÃ³a repo:

```bash
lakectl repo delete lakefs://myrepo
```

### Xem dá»¯ liá»‡u trong nhÃ¡nh:

```bash
lakectl fs ls lakefs://myrepo/main/
```

### Upload file:

```bash
lakectl fs upload --source /upload/students.csv lakefs://myrepo/main/students.csv
```

### Upload cáº£ folder:

```bash
lakectl fs upload --recursive --source /upload/ lakefs://myrepo/main/rawdata/
```

### \$ <img src="images/upload files.png" alt="upload files" width="600"/>

### XoÃ¡ file / thÆ° má»¥c:

```bash
lakectl fs rm lakefs://myrepo/main/students.csv
lakectl fs rm --recursive lakefs://myrepo/main/rawdata/
```

### Commit thay Ä‘á»•i:

```bash
lakectl commit lakefs://myrepo/dev -m "upd"
```

### \$ <img src="images/branch commit.png" alt="branch commit" width="600"/>

### NhÃ¡nh (branch):

```bash
lakectl branch create lakefs://myrepo/dev --source lakefs://myrepo/main
```

### \$ <img src="images/create new branch.png" alt="create new branch" width="600"/>

### Táº¡o thÆ° má»¥c báº±ng upload file:

```bash
echo "ghi chu" > issue.txt
lakectl fs upload --source issue.txt lakefs://myrepo/dev/processed-data/issue.txt
```

### Reset thay Ä‘á»•i chÆ°a commit:

```bash
lakectl branch reset lakefs://myrepo/dev
```

### Rollback commit:

```bash
lakectl branch revert lakefs://myrepo/main <commit-id>
```

### Kiá»ƒm tra commit hiá»‡n táº¡i:

```bash
lakectl branch show lakefs://myrepo/main
```

### So sÃ¡nh vÃ  merge:

```bash
lakectl diff lakefs://myrepo/dev                        # Xem thay Ä‘á»•i cá»§a nhÃ¡nh
lakectl merge lakefs://myrepo/dev lakefs://myrepo/main  # Merge vÃ o main, source-dest
```

### \$ <img src="images/merge.png" alt="merge" width="600"/>

---

## 4. Káº¿t ná»‘i Apache Spark

**Phá»¥ thuá»™c:**

```bash
--packages org.apache.hadoop:hadoop-aws:3.3.2
```

**Thiáº¿t láº­p config:**

```bash
spark.hadoop.fs.s3a.access.key=admin
spark.hadoop.fs.s3a.secret.key=Admin12345
spark.hadoop.fs.s3a.endpoint=http://minio:9000
spark.hadoop.fs.s3a.path.style.access=true
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
```

---

## 5. Xá»­ lÃ½ vá»›i Spark

**Äá»c file:**

```python
spark.read.csv("s3a://mybucket/file.csv", header=True)
```

**Ghi káº¿t quáº£:**

```python
df.write.csv("s3a://mybucket/processed/")
```

---

## 6. Delta Lake

**Phá»¥ thuá»™c:**

```bash
--packages io.delta:delta-core_2.12:2.4.0
```

**Khá»Ÿi táº¡o SparkSession:**

```python
spark = SparkSession.builder \
    .appName("DeltaLake Integration") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .getOrCreate()
```

**Ghi vÃ  Ä‘á»c:**

```python
df.write.format("delta").save("s3a://mybucket/delta-table")
delta_df = spark.read.format("delta").load("s3a://mybucket/delta-table")
```

**Time travel:**

```python
spark.read.format("delta").option("versionAsOf", 0).load("s3a://mybucket/delta-table")
```

---

## Cáº¥u trÃºc dá»± Ã¡n

```text
demo/
â”œâ”€â”€ data/             # Dá»¯ liá»‡u upload
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env              # ThÃ´ng tin credentials
â”œâ”€â”€ lakectl.yaml      # Cáº¥u hÃ¬nh CLI lakectl
â””â”€â”€ README.md         # File hÆ°á»›ng dáº«n
```
